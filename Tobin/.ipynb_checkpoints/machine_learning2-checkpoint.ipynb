{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model\n",
    "## uses dummy variables to avoid missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'all_neuron_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a58e14ada9ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneuro\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all_neuron_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'all_neuron_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "neuro = pd.read_csv(\"all_neuron_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign all hippocampus neurons an area of Hippocampus\n",
    "words = [\"CA1\", \"CA3\"]\n",
    "for i, row in neuro.iterrows():\n",
    "    if any(word in row[\"Cell Type\"] for word in words):\n",
    "#         print(\"true\")0\n",
    "        neuro.loc[i, \"Area\"] = \"Hippocampus\"\n",
    "#         row[\"Area\"] = \"Hippocampus\"\n",
    "    else:\n",
    "        neuro.loc[i, \"Area\"] = \"other\"\n",
    "        row[\"Area\"] = \"other\"\n",
    "#     print(row[\"Cell Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_df = pd.get_dummies(neuro, columns=[\"Measurement\"])\n",
    "categorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hippo_df = categorized_df.loc[categorized_df[\"Area\"] == \"Hippocampus\"]\n",
    "X=hippo_df.drop([\"Cell Type\", \"Area\"], axis=1)\n",
    "y=hippo_df[\"Cell Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Al\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1, stratify=y)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=53))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=13, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " - 1s - loss: 1.7849 - acc: 0.5990\n",
      "Epoch 2/60\n",
      " - 0s - loss: 1.3934 - acc: 0.6396\n",
      "Epoch 3/60\n",
      " - 0s - loss: 1.3231 - acc: 0.6417\n",
      "Epoch 4/60\n",
      " - 0s - loss: 1.3016 - acc: 0.6396\n",
      "Epoch 5/60\n",
      " - 0s - loss: 1.2848 - acc: 0.6403\n",
      "Epoch 6/60\n",
      " - 0s - loss: 1.2702 - acc: 0.6417\n",
      "Epoch 7/60\n",
      " - 0s - loss: 1.2643 - acc: 0.6382\n",
      "Epoch 8/60\n",
      " - 0s - loss: 1.2546 - acc: 0.6425\n",
      "Epoch 9/60\n",
      " - 0s - loss: 1.2605 - acc: 0.6389\n",
      "Epoch 10/60\n",
      " - 0s - loss: 1.2437 - acc: 0.6417\n",
      "Epoch 11/60\n",
      " - 0s - loss: 1.2455 - acc: 0.6375\n",
      "Epoch 12/60\n",
      " - 0s - loss: 1.2406 - acc: 0.6396\n",
      "Epoch 13/60\n",
      " - 0s - loss: 1.2400 - acc: 0.6403\n",
      "Epoch 14/60\n",
      " - 0s - loss: 1.2342 - acc: 0.6425\n",
      "Epoch 15/60\n",
      " - 0s - loss: 1.2324 - acc: 0.6403\n",
      "Epoch 16/60\n",
      " - 0s - loss: 1.2235 - acc: 0.6417\n",
      "Epoch 17/60\n",
      " - 0s - loss: 1.2310 - acc: 0.6432\n",
      "Epoch 18/60\n",
      " - 0s - loss: 1.2218 - acc: 0.6417\n",
      "Epoch 19/60\n",
      " - 0s - loss: 1.2254 - acc: 0.6410\n",
      "Epoch 20/60\n",
      " - 0s - loss: 1.2213 - acc: 0.6439\n",
      "Epoch 21/60\n",
      " - 0s - loss: 1.2201 - acc: 0.6417\n",
      "Epoch 22/60\n",
      " - 0s - loss: 1.2194 - acc: 0.6410\n",
      "Epoch 23/60\n",
      " - 0s - loss: 1.2163 - acc: 0.6460\n",
      "Epoch 24/60\n",
      " - 0s - loss: 1.2166 - acc: 0.6403\n",
      "Epoch 25/60\n",
      " - 0s - loss: 1.2128 - acc: 0.6432\n",
      "Epoch 26/60\n",
      " - 0s - loss: 1.2186 - acc: 0.6432\n",
      "Epoch 27/60\n",
      " - 0s - loss: 1.2198 - acc: 0.6353\n",
      "Epoch 28/60\n",
      " - 0s - loss: 1.2112 - acc: 0.6417\n",
      "Epoch 29/60\n",
      " - 0s - loss: 1.2117 - acc: 0.6410\n",
      "Epoch 30/60\n",
      " - 0s - loss: 1.2086 - acc: 0.6403\n",
      "Epoch 31/60\n",
      " - 0s - loss: 1.2097 - acc: 0.6417\n",
      "Epoch 32/60\n",
      " - 0s - loss: 1.2070 - acc: 0.6425\n",
      "Epoch 33/60\n",
      " - 0s - loss: 1.2068 - acc: 0.6417\n",
      "Epoch 34/60\n",
      " - 0s - loss: 1.2057 - acc: 0.6439\n",
      "Epoch 35/60\n",
      " - 0s - loss: 1.2061 - acc: 0.6439\n",
      "Epoch 36/60\n",
      " - 0s - loss: 1.2002 - acc: 0.6432\n",
      "Epoch 37/60\n",
      " - 0s - loss: 1.2032 - acc: 0.6432\n",
      "Epoch 38/60\n",
      " - 0s - loss: 1.2067 - acc: 0.6396\n",
      "Epoch 39/60\n",
      " - 0s - loss: 1.1992 - acc: 0.6432\n",
      "Epoch 40/60\n",
      " - 0s - loss: 1.1984 - acc: 0.6446\n",
      "Epoch 41/60\n",
      " - 0s - loss: 1.2024 - acc: 0.6481\n",
      "Epoch 42/60\n",
      " - 0s - loss: 1.1981 - acc: 0.6439\n",
      "Epoch 43/60\n",
      " - 0s - loss: 1.2003 - acc: 0.6396\n",
      "Epoch 44/60\n",
      " - 0s - loss: 1.2019 - acc: 0.6432\n",
      "Epoch 45/60\n",
      " - 0s - loss: 1.1977 - acc: 0.6425\n",
      "Epoch 46/60\n",
      " - 0s - loss: 1.1945 - acc: 0.6467\n",
      "Epoch 47/60\n",
      " - 0s - loss: 1.2002 - acc: 0.6439\n",
      "Epoch 48/60\n",
      " - 0s - loss: 1.1943 - acc: 0.6460\n",
      "Epoch 49/60\n",
      " - 0s - loss: 1.1928 - acc: 0.6467\n",
      "Epoch 50/60\n",
      " - 0s - loss: 1.2009 - acc: 0.6460\n",
      "Epoch 51/60\n",
      " - 0s - loss: 1.1918 - acc: 0.6460\n",
      "Epoch 52/60\n",
      " - 0s - loss: 1.1916 - acc: 0.6432\n",
      "Epoch 53/60\n",
      " - 0s - loss: 1.1941 - acc: 0.6474\n",
      "Epoch 54/60\n",
      " - 0s - loss: 1.1929 - acc: 0.6474\n",
      "Epoch 55/60\n",
      " - 0s - loss: 1.1880 - acc: 0.6460\n",
      "Epoch 56/60\n",
      " - 0s - loss: 1.1888 - acc: 0.6453\n",
      "Epoch 57/60\n",
      " - 0s - loss: 1.1867 - acc: 0.6481\n",
      "Epoch 58/60\n",
      " - 0s - loss: 1.1870 - acc: 0.6460\n",
      "Epoch 59/60\n",
      " - 0s - loss: 1.1857 - acc: 0.6460\n",
      "Epoch 60/60\n",
      " - 0s - loss: 1.1874 - acc: 0.6467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x232997d7da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 1.4494285680083578, Accuracy: 0.6439232422090543\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"machine_learning2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Al\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "encoded_predictions = model.predict_classes(X_test_scaled[:5])\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: ['Hippocampus CA1 pyramidal cell' 'Hippocampus CA1 pyramidal cell'\n",
      " 'Hippocampus CA1 pyramidal cell' 'Hippocampus CA1 pyramidal cell'\n",
      " 'Hippocampus CA1 pyramidal cell']\n",
      "Actual Labels: ['Hippocampus CA1 pyramidal cell', 'Hippocampus CA1 pyramidal cell', 'Hippocampus CA3 pyramidal cell', 'Hippocampus CA1 pyramidal cell', 'Hippocampus CA1 pyramidal cell']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {list(y_test[:5])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
